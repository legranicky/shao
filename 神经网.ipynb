{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 添加层\n",
    "def add_layer(inputs, in_size, out_size, activation_function=None):\n",
    "    # add one more layer and return the output of this layer\n",
    "    Weights = tf.Variable(tf.random_normal([in_size, out_size]))\n",
    "    biases = tf.Variable(tf.zeros([1, out_size]) + 0.1)\n",
    "    Wx_plus_b = tf.matmul(inputs, Weights) + biases\n",
    "    if activation_function is None:\n",
    "        outputs = Wx_plus_b\n",
    "    else:\n",
    "        outputs = activation_function(Wx_plus_b)\n",
    "    return outputs\n",
    "\n",
    "# 1.训练的数据\n",
    "# Make up some real data \n",
    "x_data = np.linspace(-1,1,300)[:, np.newaxis]\n",
    "noise = np.random.normal(0, 0.05, x_data.shape)\n",
    "y_data = np.square(x_data) - 0.5 + noise\n",
    "\n",
    "# 2.定义节点准备接收数据\n",
    "# define placeholder for inputs to network  \n",
    "xs = tf.placeholder(tf.float32, [None, 1])\n",
    "ys = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "# 3.定义神经层：隐藏层和预测层\n",
    "# add hidden layer 输入值是 xs，在隐藏层有 10 个神经元   \n",
    "l1 = add_layer(xs, 1, 10, activation_function=tf.nn.relu)\n",
    "# add output layer 输入值是隐藏层 l1，在预测层输出 1 个结果\n",
    "prediction = add_layer(l1, 10, 1, activation_function=None)\n",
    "\n",
    "# 4.定义 loss 表达式\n",
    "# the error between prediciton and real data    \n",
    "loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction),\n",
    "                     reduction_indices=[1]))\n",
    "\n",
    "# 5.选择 optimizer 使 loss 达到最小                   \n",
    "# 这一行定义了用什么方式去减少 loss，学习率是 0.1       \n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "\n",
    "\n",
    "# important step 对所有变量进行初始化\n",
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "# 上面定义的都没有运算，直到 sess.run 才会开始运算\n",
    "sess.run(init)\n",
    "\n",
    "# 迭代 1000 次学习，sess.run optimizer\n",
    "for i in range(1000):\n",
    "    # training train_step 和 loss 都是由 placeholder 定义的运算，所以这里要用 feed 传入参数\n",
    "    sess.run(train_step, feed_dict={xs: x_data, ys: y_data})\n",
    "    if i % 50 == 0:\n",
    "        # to see the step improvement\n",
    "        print(sess.run(loss, feed_dict={xs: x_data, ys: y_data}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#详细步骤\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#导入或者随机定义训练的数据 x 和 y：\n",
    "x_data = np.random.rand(100).astype(np.float32)\n",
    "y_data = x_data*0.1 + 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#先定义出参数 Weights，biases，拟合公式 y，误差公式 loss：\n",
    "Weights = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "biases = tf.Variable(tf.zeros([1]))\n",
    "y = Weights*x_data + biases\n",
    "loss = tf.reduce_mean(tf.square(y-y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#选择 Gradient Descent 这个最基本的 Optimizer：\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#神经网络的 key idea，就是让 loss 达到最小：\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#前面是定义，在运行模型前先要初始化所有变量：\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#接下来把结构激活，sesseion像一个指针指向要处理的地方：\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#init 就被激活了，不要忘记激活：\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#训练201步：\n",
    "for step in range(201):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#要训练 train，也就是 optimizer：\n",
    "sess.run(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#每 20 步打印一下结果，sess.run 指向 Weights，biases 并被输出：\n",
    "if step % 20 == 0:\n",
    "print(step, sess.run(Weights), sess.run(biases))\n",
    "#所以关键的就是 y，loss，optimizer 是如何定义的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#矩阵乘法：tf.matmul\n",
    "product = tf.matmul(matrix1, matrix2) # matrix multiply np.dot(m1, m2)\n",
    "#定义 Session，它是个对象，注意大写：\n",
    "sess = tf.Session()\n",
    "#result 要去 sess.run 那里取结果：\n",
    "result = sess.run(product)\n",
    "\n",
    "#用 tf.Variable 定义变量，与python不同的是，必须先定义它是一个变量，它才是一个变量，初始值为0，还可以给它一个名字 counter：\n",
    "state = tf.Variable(0, name='counter')\n",
    "#将 new_value 加载到 state 上，counter就被更新：\n",
    "update = tf.assign(state, new_value)\n",
    "#如果有变量就一定要做初始化：\n",
    "init = tf.initialize_all_variables() # must have if define variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#要给节点输入数据时用 placeholder，在 TensorFlow 中用placeholder 来描述等待输入的节点，只需要指定类型即可，然后在执行节点的时候用一个字典来“喂”这些节点。相当于先把变量 hold 住，然后每次从外部传入data，注意 placeholder 和 feed_dict 是绑定用的。\n",
    "#这里简单提一下 feed 机制， 给 feed 提供数据，作为 run()调用的参数， feed 只在调用它的方法内有效, 方法结束, feed 就会消失。\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "input1 = tf.placeholder(tf.float32)\n",
    "input2 = tf.placeholder(tf.float32)\n",
    "ouput = tf.mul(input1, input2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  print(sess.run(ouput, feed_dict={input1: [7.], input2: [2.]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#神经网络基本概念\n",
    "#激励函数：例如一个神经元对猫的眼睛敏感，那当它看到猫的眼睛的时候，就被激励了，相应的参数就会被调优，它的贡献就会越大。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#添加神经层：输入参数有 inputs, in_size, out_size, 和 activation_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def add_layer(inputs, in_size, out_size,  activation_function=None):\n",
    "\n",
    "  Weights = tf.Variable(tf.random_normal([in_size, out_size]))\n",
    "  biases = tf.Variable(tf.zeros([1, out_size]) + 0.1)\n",
    "  Wx_plus_b = tf.matmul(inputs, Weights) + biases\n",
    "\n",
    "  if activation_function is None:\n",
    "    outputs = Wx_plus_b\n",
    "  else:\n",
    "    outputs = activation_function(Wx_plus_b)\n",
    "\n",
    "return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#分类问题的 loss 函数 cross_entropy ：\n",
    "# the error between prediction and real data\n",
    "# loss 函数用 cross entropy\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction),\n",
    "                                              reduction_indices=[1]))       # loss\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Tensorflow 有一个很好的工具, 叫做dropout, 只需要给予它一个不被 drop 掉的百分比，就能很好地降低 overfitting。\n",
    "#dropout 是指在深度学习网络的训练过程中，按照一定的概率将一部分神经网络单元暂时从网络中丢弃，相当于从原始的网络中找到一个更瘦的网络\n",
    "#代码实现就是在 add layer 函数里加上 dropout, keep_prob 就是保持多少不被 drop，在迭代时在 sess.run 中被 feed:\n",
    "\n",
    "def add_layer(inputs, in_size, out_size, layer_name, activation_function=None, ):\n",
    "    # add one more layer and return the output of this layer\n",
    "    Weights = tf.Variable(tf.random_normal([in_size, out_size]))\n",
    "    biases = tf.Variable(tf.zeros([1, out_size]) + 0.1, )\n",
    "    Wx_plus_b = tf.matmul(inputs, Weights) + biases\n",
    "\n",
    "    # here to dropout\n",
    "    # 在 Wx_plus_b 上drop掉一定比例\n",
    "    # keep_prob 保持多少不被drop，在迭代时在 sess.run 中 feed\n",
    "    Wx_plus_b = tf.nn.dropout(Wx_plus_b, keep_prob)\n",
    "\n",
    "    if activation_function is None:\n",
    "        outputs = Wx_plus_b\n",
    "    else:\n",
    "        outputs = activation_function(Wx_plus_b, )\n",
    "    tf.histogram_summary(layer_name + '/outputs', outputs)  \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#训练好了一个神经网络后，可以保存起来下次使用时再次加载：\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "## Save to file\n",
    "# remember to define the same dtype and shape when restore\n",
    "W = tf.Variable([[1,2,3],[3,4,5]], dtype=tf.float32, name='weights')\n",
    "b = tf.Variable([[1,2,3]], dtype=tf.float32, name='biases')\n",
    "\n",
    "init= tf.initialize_all_variables()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# 用 saver 将所有的 variable 保存到定义的路径\n",
    "with tf.Session() as sess:\n",
    "   sess.run(init)\n",
    "   save_path = saver.save(sess, \"my_net/save_net.ckpt\")\n",
    "   print(\"Save to path: \", save_path)\n",
    "\n",
    "\n",
    "################################################\n",
    "\n",
    "# restore variables\n",
    "# redefine the same shape and same type for your variables\n",
    "W = tf.Variable(np.arange(6).reshape((2, 3)), dtype=tf.float32, name=\"weights\")\n",
    "b = tf.Variable(np.arange(3).reshape((1, 3)), dtype=tf.float32, name=\"biases\")\n",
    "\n",
    "# not need init step\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "# 用 saver 从路径中将 save_net.ckpt 保存的 W 和 b restore 进来\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"my_net/save_net.ckpt\")\n",
    "    print(\"weights:\", sess.run(W))\n",
    "    print(\"biases:\", sess.run(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
